{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "OHJWesKs-tqd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "import pickle\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from util import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAghP_o0-tqe"
      },
      "source": [
        "Reads Config file and prepares the arguments you can choose in the config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "BbUZJ2E--tqe"
      },
      "outputs": [],
      "source": [
        "LR_DECAY = False\n",
        "def get_config():\n",
        "\n",
        "    # read config json and update the sysarg\n",
        "    with open(\"config.json\", \"r\") as read_file:\n",
        "        config = json.load(read_file)\n",
        "\n",
        "    if config[\"config_override\"] == \"\":\n",
        "        del config['config_override']\n",
        "    else:\n",
        "        print(config['config_override'])\n",
        "        config_override = json.loads(config['config_override'])\n",
        "        del config['config_override']\n",
        "        config.update(config_override)\n",
        "\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-J1YEoM-tqe"
      },
      "source": [
        "Class SimpleLinear with simple MLP for MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "id5Wyt-V-tqf"
      },
      "outputs": [],
      "source": [
        "class SimpleLinear(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, h1=2048):\n",
        "        super().__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, h1)\n",
        "        self.fc2 = torch.nn.Linear(h1, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # def weight(self):\n",
        "    #     return self.linear1.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qFBH01M-tqf"
      },
      "source": [
        "Class TrainMNISTCluster with all the methods needed to run the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkGiGQ2G-tqf"
      },
      "outputs": [],
      "source": [
        "class TrainMNISTCluster(object):\n",
        "    def __init__(self, config, device):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        assert self.config['m'] % self.config['p'] == 0\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
        "\n",
        "        self.result_fname = os.path.join(self.config['project_dir'], 'results.pickle')\n",
        "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint.pt')\n",
        "\n",
        "        self.setup_datasets()\n",
        "        self.setup_models()\n",
        "\n",
        "        self.epoch = None\n",
        "        self.lr = None\n",
        "\n",
        "\n",
        "    def setup_datasets(self):\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        # generate indices for each dataset\n",
        "        # also write cluster info\n",
        "\n",
        "        MNIST_TRAINSET_DATA_SIZE = 60000\n",
        "        MNIST_TESTSET_DATA_SIZE = 10000\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        self.dataset = {}\n",
        "\n",
        "        if cfg['uneven'] == True:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=True)\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        else:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'], random=True)\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def _setup_dataset_random_n(self, num_data, p, m, n, random = True):\n",
        "\n",
        "        print(\"m:\",m)\n",
        "        print(\"p:\",p)\n",
        "        print(\"num_data:\",num_data)\n",
        "\n",
        "        dataset = {}\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        data_indices = []\n",
        "        cluster_assign = []\n",
        "\n",
        "        m_per_cluster = m // p\n",
        "\n",
        "        for p_i in range(p):\n",
        "\n",
        "            ll = list(np.random.permutation(num_data))\n",
        "\n",
        "            ll2 = chunkify_uneven(ll, m_per_cluster) # splits ll into m lists\n",
        "            data_indices += ll2\n",
        "\n",
        "            cluster_assign += [p_i for _ in range(m_per_cluster)]\n",
        "\n",
        "        data_indices = np.array(data_indices, dtype=object)\n",
        "        cluster_assign = np.array(cluster_assign)\n",
        "        assert data_indices.shape[0] == cluster_assign.shape[0]\n",
        "        assert data_indices.shape[0] == m\n",
        "\n",
        "\n",
        "        return data_indices, cluster_assign\n",
        "\n",
        "\n",
        "    def _load_MNIST(self, train=True):\n",
        "        transforms = torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               # torchvision.transforms.Normalize(\n",
        "                               #   (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "        if train:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
        "        else:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
        "\n",
        "        dl = DataLoader(mnist_dataset)\n",
        "\n",
        "        X = dl.dataset.data # (60000,28, 28)\n",
        "        y = dl.dataset.targets #(60000)\n",
        "\n",
        "        # normalize to have 0 ~ 1 range in each pixel\n",
        "\n",
        "        X = X / 255.0\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    # Need p models for each client\n",
        "\n",
        "    def setup_models(self):\n",
        "        np.random.seed(self.config['train_seed'])\n",
        "        torch.manual_seed(self.config['train_seed'])\n",
        "\n",
        "        p = self.config['p']\n",
        "        m = self.config['m']\n",
        "\n",
        "        self.models = [[SimpleLinear(h1 = self.config['h1']).to(self.device) for p_i in range(p)] for m_i in range(m)] # p models with p different params of dimension(1,d) for each client m_i\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        num_epochs = self.config['num_epochs']\n",
        "        lr = self.config['lr']\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # epoch -1\n",
        "        self.epoch = -1\n",
        "\n",
        "        result = {}\n",
        "        result['epoch'] = -1\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=True)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['train'] = res\n",
        "\n",
        "        self.print_epoch_stats(res)\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=False)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['test'] = res\n",
        "        self.print_epoch_stats(res)\n",
        "        results.append(result)\n",
        "\n",
        "        # this will be used in next epoch\n",
        "        cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.epoch = epoch\n",
        "\n",
        "            result = {}\n",
        "            result['epoch'] = epoch\n",
        "\n",
        "            lr = self.lr_schedule(epoch)\n",
        "            result['lr'] = lr\n",
        "\n",
        "            t0 = time.time()\n",
        "            result['train'] = self.train(cluster_assign, lr = lr)\n",
        "            t1 = time.time()\n",
        "            train_time = t1-t0\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=True)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            res['train_time'] = train_time\n",
        "            res['lr'] = lr\n",
        "            result['train'] = res\n",
        "\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=False)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            result['test'] = res\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "            # this will be used in next epoch's gradient update\n",
        "            #if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
        "            cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
        "                with open(self.result_fname, 'wb') as outfile:\n",
        "                    pickle.dump(results, outfile)\n",
        "                    print(f'result written at {self.result_fname}')\n",
        "#                self.save_checkpoint()\n",
        "                print(f'checkpoint written at {self.checkpoint_fname}')\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['train']['loss'] for r in results], label='train')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('loss')\n",
        "        plt.title('Training Loss per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'train_loss.png'))\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['test']['acc'] for r in results], label='train')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('test accuracy')\n",
        "        plt.title('Test Accuracy per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'test_acc.png'))\n",
        "\n",
        "\n",
        "    def lr_schedule(self, epoch):\n",
        "        if self.lr is None:\n",
        "            self.lr = self.config['lr']\n",
        "\n",
        "        if epoch % 50 == 0 and epoch != 0 and LR_DECAY:\n",
        "            self.lr = self.lr * 0.1\n",
        "\n",
        "        return self.lr        \n",
        "\n",
        "\n",
        "    def print_epoch_stats(self, res):\n",
        "        if res['is_train']:\n",
        "            data_str = 'tr'\n",
        "        else:\n",
        "            data_str = 'tst'\n",
        "\n",
        "        if 'train_time' in res:\n",
        "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
        "        else:\n",
        "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
        "\n",
        "        if 'lr' in res:\n",
        "            lr_str = f\" lr {res['lr']:4f}\"\n",
        "        else:\n",
        "            lr_str = \"\"\n",
        "\n",
        "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} clct{res['cl_ct']}{lr_str} {time_str}\"\n",
        "\n",
        "        print(str0)\n",
        "\n",
        "    def train(self, cluster_assign, lr):\n",
        "        VERBOSE = 0\n",
        "\n",
        "        cfg = self.config\n",
        "        m = cfg['m']\n",
        "        p = cfg['p']\n",
        "        tau = cfg['tau']\n",
        "\n",
        "        # run local update\n",
        "        t0 = time.time()\n",
        "\n",
        "\n",
        "        for m_i in range(m):\n",
        "            if VERBOSE and m_i % 100 == 0: print(f'm {m_i}/{m} processing \\r', end ='')\n",
        "\n",
        "            (X, y) = self.load_data(m_i)\n",
        "\n",
        "            p_i = cluster_assign[m_i]\n",
        "            model = self.models[m_i][p_i]\n",
        "\n",
        "            # LOCAL UPDATE PER MACHINE tau times\n",
        "            for step_i in range(tau):\n",
        "\n",
        "                y_logit = model(X)\n",
        "                loss = self.criterion(y_logit, y)\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "                self.local_param_update(model, lr)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "\n",
        "        t02 = time.time()\n",
        "        # print(f'running single ..took {t02-t01:.3f}sec')\n",
        "\n",
        "\n",
        "        t1 = time.time()\n",
        "        if VERBOSE: print(f'local update {t1-t0:.3f}sec')\n",
        "\n",
        "        # apply gradient update\n",
        "        t0 = time.time()\n",
        "\n",
        "        # NEEDS TO BE DECENTRALIZED\n",
        "        self.dec_param_update(cluster_assign)\n",
        "        t1 = time.time()\n",
        "\n",
        "        if VERBOSE: print(f'global update {t1-t0:.3f}sec')\n",
        "\n",
        "    def check_local_model_loss(self, local_models):\n",
        "        # for debugging\n",
        "        m = self.config['m']\n",
        "\n",
        "        losses = []\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i)\n",
        "            y_logit = local_models[m_i](X)\n",
        "            loss = self.criterion(y_logit, y)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        return np.array(losses)\n",
        "\n",
        "\n",
        "    def get_inference_stats(self, train = True):\n",
        "        cfg = self.config\n",
        "        if train:\n",
        "            m = cfg['m']\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            m = cfg['m_test']\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        p = cfg['p']\n",
        "\n",
        "\n",
        "        num_data = 0\n",
        "        losses = {}\n",
        "        corrects = {}\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i, train=train) # load batch data rotated\n",
        "\n",
        "            for p_i in range(p):\n",
        "                y_logit = self.models[m_i][p_i](X)\n",
        "                loss = self.criterion(y_logit, y) # loss of\n",
        "                n_correct = self.n_correct(y_logit, y)\n",
        "\n",
        "                # if torch.isnan(loss):\n",
        "                #     print(\"nan loss: \", dataset['data_indices'][m_i])\n",
        "\n",
        "                losses[(m_i,p_i)] = loss.item()\n",
        "                corrects[(m_i,p_i)] = n_correct\n",
        "\n",
        "            num_data += X.shape[0]\n",
        "\n",
        "        # calculate loss and cluster the machines\n",
        "        cluster_assign = []\n",
        "        for m_i in range(m):\n",
        "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
        "            #print(\"Machine Losses:\", machine_losses)\n",
        "            min_p_i = np.argmin(machine_losses)\n",
        "            cluster_assign.append(min_p_i)\n",
        "\n",
        "        # calculate optimal model's loss, acc over all models\n",
        "        min_corrects = []\n",
        "        min_losses = []\n",
        "        for m_i, p_i in enumerate(cluster_assign):\n",
        "\n",
        "            min_loss = losses[(m_i,p_i)]\n",
        "            min_losses.append(min_loss)\n",
        "\n",
        "            min_correct = corrects[(m_i,p_i)]\n",
        "            min_corrects.append(min_correct)\n",
        "\n",
        "        # print(\"losses: \", min_losses)\n",
        "        loss = np.mean(min_losses)\n",
        "        acc = np.sum(min_corrects) / num_data\n",
        "\n",
        "\n",
        "        # check cluster assignment acc\n",
        "        cl_acc = np.mean(np.array(cluster_assign) == np.array(dataset['cluster_assign']))\n",
        "        cl_ct = [np.sum(np.array(cluster_assign) == p_i ) for p_i in range(p)]\n",
        "\n",
        "        res = {} # results\n",
        "        # res['losses'] = losses\n",
        "        # res['corrects'] = corrects\n",
        "        res['cluster_assign'] = cluster_assign\n",
        "        res['num_data'] = num_data\n",
        "        res['loss'] = loss\n",
        "        res['acc'] = acc\n",
        "        res['cl_acc'] = cl_acc\n",
        "        res['cl_ct'] = cl_ct\n",
        "        res['is_train'] = train\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        return res\n",
        "\n",
        "    def n_correct(self, y_logit, y):\n",
        "        _, predicted = torch.max(y_logit.data, 1)\n",
        "        correct = (predicted == y).sum().item()\n",
        "\n",
        "        return correct\n",
        "\n",
        "    # TODO Does every Cluster get 4 clients with the same data, but rotated differently?\n",
        "\n",
        "    def load_data(self, m_i, train=True):\n",
        "        # this part is very fast since its just rearranging models\n",
        "        cfg = self.config\n",
        "\n",
        "        if train:\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        indices = dataset['data_indices'][m_i]\n",
        "        p_i = dataset['cluster_assign'][m_i]\n",
        "\n",
        "        X_batch = dataset['X'][indices]\n",
        "        y_batch = dataset['y'][indices]\n",
        "\n",
        "        # k : how many times rotate 90 degree\n",
        "        # k =1 : 90 , k=2 180, k=3 270\n",
        "\n",
        "        if cfg['p'] == 4:\n",
        "            k = p_i\n",
        "        elif cfg['p'] == 2:\n",
        "            k = (p_i % 2) * 2\n",
        "        elif cfg['p'] == 1:\n",
        "            k = 0\n",
        "        else:\n",
        "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
        "\n",
        "        X_batch2 = torch.rot90(X_batch, k=int(k), dims = (1,2))\n",
        "        X_batch3 = X_batch2.reshape(-1, 28 * 28)\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        return X_batch3, y_batch\n",
        "\n",
        "\n",
        "    def local_param_update(self, model, lr):\n",
        "\n",
        "        # gradient update manually\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data -= lr * param.grad\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace() # we need to check the output of name, check if duplicate exists\n",
        "\n",
        "\n",
        "    def dec_param_update(self, cluster_assign):\n",
        "\n",
        "        num_clients = self.config['m']\n",
        "\n",
        "        if num_clients <= 4:\n",
        "            return\n",
        "\n",
        "        max_e = 50\n",
        "        if num_clients <= max_e:\n",
        "            e = num_clients - 1\n",
        "        else:\n",
        "            e = min(max_e, int(np.log(num_clients) * 20))\n",
        "\n",
        "        if e >= num_clients:\n",
        "            e = num_clients - 1\n",
        "\n",
        "        client_indices = list(range(num_clients))\n",
        "\n",
        "        for m_i in range(num_clients):\n",
        "            selected_clients = random.sample([i for i in client_indices if i != m_i and cluster_assign[m_i] != cluster_assign[i]], int(np.floor(e/2)))\n",
        "            selected_clients += random.sample([i for i in client_indices if i != m_i and cluster_assign[m_i] == cluster_assign[i]], int(np.floor(e/2)))\n",
        "            m_i_cluster = cluster_assign[m_i]\n",
        "            for m_j in selected_clients:\n",
        "                m_j_cluster = cluster_assign[m_j]\n",
        "\n",
        "                m_j_params = dict(self.models[m_j][m_j_cluster].named_parameters())\n",
        "\n",
        "                if m_i_cluster == m_j_cluster:\n",
        "                    for name, param in self.models[m_i][m_i_cluster].named_parameters():\n",
        "                        m_i_param = param.data.clone()\n",
        "                        m_j_param = m_j_params[name].data.clone()\n",
        "                        alpha = 0.6\n",
        "                        param.data = (m_i_param + m_j_param) / 2     \n",
        "\n",
        "                else:\n",
        "                    for name, param in self.models[m_i][m_i_cluster].named_parameters():\n",
        "                        m_i_param = param.data.clone()\n",
        "                        m_j_param = m_j_params[name].data.clone()\n",
        "                        alpha = 0.6\n",
        "                        param.data = alpha * m_i_param + (1 - alpha) * m_j_param\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def test(self, train=False):\n",
        "        return self.get_inference_stats(train=train)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        models_to_save = [model.state_dict() for model in self.models]\n",
        "        torch.save({'models':models_to_save}, self.checkpoint_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADsUSUi-tqf"
      },
      "source": [
        "Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T_XDv25r-tqf",
        "outputId": "9c8f4300-c792-4e49-be40-c694fa066e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config: {'m': 800, 'm_test': 80, 'p': 4, 'n': 400, 'uneven': True, \"k'\": 2, 'h1': 200, 'num_epochs': 300, 'batch_size': 100, 'tau': 10, 'lr': 0.1, 'data_seed': 10, 'train_seed': 10, 'project_dir': 'output'}\n",
            "Using device: cpu\n",
            "m: 800\n",
            "p: 4\n",
            "num_data: 60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "m: 80\n",
            "p: 4\n",
            "num_data: 10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "Epoch -1 tr: l 2.296 a 0.116 clct[np.int64(223), np.int64(188), np.int64(209), np.int64(180)] 0.794sec\n",
            "Epoch -1 tst: l 2.297 a 0.116 clct[np.int64(16), np.int64(21), np.int64(22), np.int64(21)] 0.110sec\n",
            "Epoch 0 tr: l 2.284 a 0.200 clct[np.int64(214), np.int64(187), np.int64(214), np.int64(185)] lr 0.100000 5.134sec(train) 0.643sec(infer)\n",
            "Epoch 0 tst: l 2.282 a 0.187 clct[np.int64(20), np.int64(18), np.int64(17), np.int64(25)] 0.086sec\n",
            "result written at output/results.pickle\n",
            "checkpoint written at output/checkpoint.pt\n",
            "Epoch 1 tr: l 2.289 a 0.189 clct[np.int64(215), np.int64(187), np.int64(203), np.int64(195)] lr 0.100000 4.821sec(train) 0.630sec(infer)\n",
            "Epoch 1 tst: l 2.282 a 0.179 clct[np.int64(19), np.int64(18), np.int64(16), np.int64(27)] 0.083sec\n",
            "Epoch 2 tr: l 2.286 a 0.171 clct[np.int64(217), np.int64(188), np.int64(193), np.int64(202)] lr 0.100000 4.739sec(train) 0.629sec(infer)\n",
            "Epoch 2 tst: l 2.284 a 0.190 clct[np.int64(18), np.int64(18), np.int64(17), np.int64(27)] 0.082sec\n",
            "Epoch 3 tr: l 2.276 a 0.146 clct[np.int64(220), np.int64(186), np.int64(194), np.int64(200)] lr 0.100000 4.750sec(train) 0.631sec(infer)\n",
            "Epoch 3 tst: l 2.275 a 0.151 clct[np.int64(20), np.int64(17), np.int64(15), np.int64(28)] 0.083sec\n",
            "Epoch 4 tr: l 2.255 a 0.151 clct[np.int64(218), np.int64(184), np.int64(194), np.int64(204)] lr 0.100000 4.804sec(train) 0.631sec(infer)\n",
            "Epoch 4 tst: l 2.259 a 0.150 clct[np.int64(16), np.int64(19), np.int64(16), np.int64(29)] 0.082sec\n",
            "Epoch 5 tr: l 2.220 a 0.180 clct[np.int64(219), np.int64(187), np.int64(192), np.int64(202)] lr 0.100000 4.770sec(train) 0.633sec(infer)\n",
            "Epoch 5 tst: l 2.229 a 0.178 clct[np.int64(17), np.int64(17), np.int64(17), np.int64(29)] 0.082sec\n",
            "Epoch 6 tr: l 2.169 a 0.239 clct[np.int64(222), np.int64(189), np.int64(188), np.int64(201)] lr 0.100000 4.760sec(train) 0.633sec(infer)\n",
            "Epoch 6 tst: l 2.174 a 0.230 clct[np.int64(16), np.int64(19), np.int64(18), np.int64(27)] 0.083sec\n",
            "Epoch 7 tr: l 2.101 a 0.307 clct[np.int64(223), np.int64(189), np.int64(188), np.int64(200)] lr 0.100000 4.772sec(train) 0.635sec(infer)\n",
            "Epoch 7 tst: l 2.106 a 0.302 clct[np.int64(16), np.int64(19), np.int64(18), np.int64(27)] 0.082sec\n",
            "Epoch 8 tr: l 2.011 a 0.369 clct[np.int64(222), np.int64(189), np.int64(189), np.int64(200)] lr 0.100000 4.864sec(train) 0.655sec(infer)\n",
            "Epoch 8 tst: l 2.018 a 0.365 clct[np.int64(17), np.int64(19), np.int64(18), np.int64(26)] 0.083sec\n",
            "Epoch 9 tr: l 1.915 a 0.404 clct[np.int64(222), np.int64(189), np.int64(189), np.int64(200)] lr 0.100000 4.773sec(train) 0.634sec(infer)\n",
            "Epoch 9 tst: l 1.927 a 0.389 clct[np.int64(16), np.int64(19), np.int64(19), np.int64(26)] 0.083sec\n",
            "Epoch 10 tr: l 1.817 a 0.426 clct[np.int64(222), np.int64(189), np.int64(189), np.int64(200)] lr 0.100000 4.758sec(train) 0.633sec(infer)\n",
            "Epoch 10 tst: l 1.825 a 0.426 clct[np.int64(16), np.int64(19), np.int64(19), np.int64(26)] 0.082sec\n",
            "result written at output/results.pickle\n",
            "checkpoint written at output/checkpoint.pt\n",
            "Epoch 11 tr: l 1.726 a 0.439 clct[np.int64(222), np.int64(189), np.int64(189), np.int64(200)] lr 0.100000 4.800sec(train) 0.654sec(infer)\n",
            "Epoch 11 tst: l 1.726 a 0.439 clct[np.int64(16), np.int64(19), np.int64(19), np.int64(26)] 0.084sec\n",
            "Epoch 12 tr: l 1.653 a 0.452 clct[np.int64(222), np.int64(189), np.int64(189), np.int64(200)] lr 0.100000 4.767sec(train) 0.631sec(infer)\n",
            "Epoch 12 tst: l 1.651 a 0.448 clct[np.int64(16), np.int64(19), np.int64(19), np.int64(26)] 0.083sec\n",
            "Epoch 13 tr: l 1.600 a 0.457 clct[np.int64(222), np.int64(189), np.int64(189), np.int64(200)] lr 0.100000 4.775sec(train) 0.632sec(infer)\n",
            "Epoch 13 tst: l 1.608 a 0.451 clct[np.int64(16), np.int64(19), np.int64(19), np.int64(26)] 0.083sec\n",
            "Epoch 14 tr: l 1.560 a 0.462 clct[np.int64(222), np.int64(189), np.int64(189), np.int64(200)] lr 0.100000 4.805sec(train) 0.634sec(infer)\n",
            "Epoch 14 tst: l 1.552 a 0.456 clct[np.int64(16), np.int64(19), np.int64(19), np.int64(26)] 0.083sec\n",
            "Epoch 15 tr: l 1.534 a 0.468 clct[np.int64(221), np.int64(191), np.int64(188), np.int64(200)] lr 0.100000 4.794sec(train) 0.651sec(infer)\n",
            "Epoch 15 tst: l 1.537 a 0.458 clct[np.int64(16), np.int64(19), np.int64(19), np.int64(26)] 0.084sec\n",
            "Epoch 16 tr: l 1.507 a 0.474 clct[np.int64(220), np.int64(192), np.int64(187), np.int64(201)] lr 0.100000 4.780sec(train) 0.633sec(infer)\n",
            "Epoch 16 tst: l 1.502 a 0.468 clct[np.int64(16), np.int64(19), np.int64(19), np.int64(26)] 0.083sec\n",
            "Epoch 17 tr: l 1.496 a 0.478 clct[np.int64(221), np.int64(191), np.int64(187), np.int64(201)] lr 0.100000 4.832sec(train) 0.631sec(infer)\n",
            "Epoch 17 tst: l 1.503 a 0.456 clct[np.int64(16), np.int64(19), np.int64(19), np.int64(26)] 0.083sec\n",
            "Epoch 18 tr: l 1.475 a 0.484 clct[np.int64(219), np.int64(191), np.int64(188), np.int64(202)] lr 0.100000 4.844sec(train) 0.669sec(infer)\n",
            "Epoch 18 tst: l 1.501 a 0.459 clct[np.int64(15), np.int64(19), np.int64(20), np.int64(26)] 0.085sec\n",
            "Epoch 19 tr: l 1.465 a 0.490 clct[np.int64(219), np.int64(191), np.int64(188), np.int64(202)] lr 0.100000 4.886sec(train) 0.635sec(infer)\n",
            "Epoch 19 tst: l 1.460 a 0.472 clct[np.int64(15), np.int64(19), np.int64(20), np.int64(26)] 0.083sec\n",
            "Epoch 20 tr: l 1.461 a 0.490 clct[np.int64(219), np.int64(192), np.int64(187), np.int64(202)] lr 0.100000 4.801sec(train) 0.635sec(infer)\n",
            "Epoch 20 tst: l 1.485 a 0.462 clct[np.int64(15), np.int64(19), np.int64(20), np.int64(26)] 0.083sec\n",
            "result written at output/results.pickle\n",
            "checkpoint written at output/checkpoint.pt\n",
            "Epoch 21 tr: l 1.450 a 0.496 clct[np.int64(218), np.int64(192), np.int64(187), np.int64(203)] lr 0.100000 4.835sec(train) 0.636sec(infer)\n",
            "Epoch 21 tst: l 1.461 a 0.480 clct[np.int64(14), np.int64(19), np.int64(21), np.int64(26)] 0.082sec\n",
            "Epoch 22 tr: l 1.445 a 0.499 clct[np.int64(216), np.int64(193), np.int64(187), np.int64(204)] lr 0.100000 4.847sec(train) 0.636sec(infer)\n",
            "Epoch 22 tst: l 1.451 a 0.493 clct[np.int64(15), np.int64(19), np.int64(20), np.int64(26)] 0.084sec\n",
            "Epoch 23 tr: l 1.438 a 0.505 clct[np.int64(211), np.int64(197), np.int64(187), np.int64(205)] lr 0.100000 4.797sec(train) 0.637sec(infer)\n",
            "Epoch 23 tst: l 1.419 a 0.513 clct[np.int64(13), np.int64(20), np.int64(20), np.int64(27)] 0.084sec\n",
            "Epoch 24 tr: l 1.428 a 0.506 clct[np.int64(205), np.int64(196), np.int64(191), np.int64(208)] lr 0.100000 4.811sec(train) 0.640sec(infer)\n",
            "Epoch 24 tst: l 1.421 a 0.502 clct[np.int64(11), np.int64(22), np.int64(20), np.int64(27)] 0.084sec\n",
            "Epoch 25 tr: l 1.417 a 0.509 clct[np.int64(192), np.int64(197), np.int64(197), np.int64(214)] lr 0.100000 4.920sec(train) 0.642sec(infer)\n",
            "Epoch 25 tst: l 1.426 a 0.503 clct[np.int64(11), np.int64(21), np.int64(20), np.int64(28)] 0.085sec\n",
            "Epoch 26 tr: l 1.407 a 0.517 clct[np.int64(186), np.int64(200), np.int64(198), np.int64(216)] lr 0.100000 4.877sec(train) 0.690sec(infer)\n",
            "Epoch 26 tst: l 1.415 a 0.508 clct[np.int64(9), np.int64(20), np.int64(24), np.int64(27)] 0.084sec\n",
            "Epoch 27 tr: l 1.388 a 0.521 clct[np.int64(180), np.int64(201), np.int64(199), np.int64(220)] lr 0.100000 4.832sec(train) 0.643sec(infer)\n",
            "Epoch 27 tst: l 1.390 a 0.518 clct[np.int64(8), np.int64(20), np.int64(24), np.int64(28)] 0.085sec\n",
            "Epoch 28 tr: l 1.377 a 0.527 clct[np.int64(181), np.int64(200), np.int64(197), np.int64(222)] lr 0.100000 4.848sec(train) 0.644sec(infer)\n",
            "Epoch 28 tst: l 1.366 a 0.529 clct[np.int64(6), np.int64(21), np.int64(20), np.int64(33)] 0.084sec\n",
            "Epoch 29 tr: l 1.376 a 0.525 clct[np.int64(178), np.int64(197), np.int64(197), np.int64(228)] lr 0.100000 4.837sec(train) 0.641sec(infer)\n",
            "Epoch 29 tst: l 1.359 a 0.531 clct[np.int64(9), np.int64(21), np.int64(21), np.int64(29)] 0.085sec\n",
            "Epoch 30 tr: l 1.370 a 0.530 clct[np.int64(176), np.int64(200), np.int64(192), np.int64(232)] lr 0.100000 4.844sec(train) 0.643sec(infer)\n",
            "Epoch 30 tst: l 1.371 a 0.521 clct[np.int64(10), np.int64(20), np.int64(20), np.int64(30)] 0.085sec\n",
            "result written at output/results.pickle\n",
            "checkpoint written at output/checkpoint.pt\n",
            "Epoch 31 tr: l 1.357 a 0.535 clct[np.int64(173), np.int64(195), np.int64(194), np.int64(238)] lr 0.100000 4.849sec(train) 0.641sec(infer)\n",
            "Epoch 31 tst: l 1.336 a 0.537 clct[np.int64(8), np.int64(14), np.int64(20), np.int64(38)] 0.084sec\n",
            "Epoch 32 tr: l 1.345 a 0.541 clct[np.int64(173), np.int64(192), np.int64(195), np.int64(240)] lr 0.100000 4.834sec(train) 0.643sec(infer)\n",
            "Epoch 32 tst: l 1.332 a 0.538 clct[np.int64(8), np.int64(17), np.int64(19), np.int64(36)] 0.084sec\n",
            "Epoch 33 tr: l 1.348 a 0.539 clct[np.int64(171), np.int64(181), np.int64(196), np.int64(252)] lr 0.100000 4.829sec(train) 0.644sec(infer)\n",
            "Epoch 33 tst: l 1.304 a 0.551 clct[np.int64(7), np.int64(17), np.int64(20), np.int64(36)] 0.085sec\n",
            "Epoch 34 tr: l 1.333 a 0.546 clct[np.int64(162), np.int64(180), np.int64(199), np.int64(259)] lr 0.100000 4.854sec(train) 0.645sec(infer)\n",
            "Epoch 34 tst: l 1.347 a 0.540 clct[np.int64(7), np.int64(17), np.int64(21), np.int64(35)] 0.084sec\n",
            "Epoch 35 tr: l 1.304 a 0.558 clct[np.int64(155), np.int64(180), np.int64(201), np.int64(264)] lr 0.100000 4.843sec(train) 0.640sec(infer)\n",
            "Epoch 35 tst: l 1.291 a 0.563 clct[np.int64(10), np.int64(15), np.int64(22), np.int64(33)] 0.084sec\n",
            "Epoch 36 tr: l 1.299 a 0.557 clct[np.int64(141), np.int64(178), np.int64(209), np.int64(272)] lr 0.100000 4.822sec(train) 0.648sec(infer)\n",
            "Epoch 36 tst: l 1.268 a 0.572 clct[np.int64(10), np.int64(14), np.int64(21), np.int64(35)] 0.083sec\n",
            "Epoch 37 tr: l 1.286 a 0.565 clct[np.int64(140), np.int64(172), np.int64(210), np.int64(278)] lr 0.100000 4.827sec(train) 0.657sec(infer)\n",
            "Epoch 37 tst: l 1.269 a 0.568 clct[np.int64(7), np.int64(13), np.int64(22), np.int64(38)] 0.084sec\n",
            "Epoch 38 tr: l 1.266 a 0.574 clct[np.int64(126), np.int64(174), np.int64(215), np.int64(285)] lr 0.100000 4.900sec(train) 0.653sec(infer)\n",
            "Epoch 38 tst: l 1.244 a 0.582 clct[np.int64(9), np.int64(12), np.int64(21), np.int64(38)] 0.084sec\n",
            "Epoch 39 tr: l 1.252 a 0.583 clct[np.int64(120), np.int64(174), np.int64(219), np.int64(287)] lr 0.100000 4.848sec(train) 0.640sec(infer)\n",
            "Epoch 39 tst: l 1.259 a 0.580 clct[np.int64(12), np.int64(11), np.int64(21), np.int64(36)] 0.085sec\n",
            "Epoch 40 tr: l 1.237 a 0.586 clct[np.int64(119), np.int64(171), np.int64(220), np.int64(290)] lr 0.100000 4.816sec(train) 0.641sec(infer)\n",
            "Epoch 40 tst: l 1.225 a 0.587 clct[np.int64(10), np.int64(9), np.int64(21), np.int64(40)] 0.084sec\n",
            "result written at output/results.pickle\n",
            "checkpoint written at output/checkpoint.pt\n",
            "Epoch 41 tr: l 1.218 a 0.592 clct[np.int64(117), np.int64(172), np.int64(220), np.int64(291)] lr 0.100000 4.845sec(train) 0.639sec(infer)\n",
            "Epoch 41 tst: l 1.219 a 0.590 clct[np.int64(8), np.int64(6), np.int64(25), np.int64(41)] 0.084sec\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[94], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainMNISTCluster(config, device)\n\u001b[1;32m     11\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m duration \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---train cluster Ended in \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;124m hour (\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m sec) \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (duration\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m3600\u001b[39m), duration))\n",
            "Cell \u001b[0;32mIn[93], line 190\u001b[0m, in \u001b[0;36mTrainMNISTCluster.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[1;32m    189\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 190\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_assign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    192\u001b[0m train_time \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
            "Cell \u001b[0;32mIn[93], line 318\u001b[0m, in \u001b[0;36mTrainMNISTCluster.train\u001b[0;34m(self, cluster_assign, lr)\u001b[0m\n\u001b[1;32m    315\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# NEEDS TO BE DECENTRALIZED\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_param_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_assign\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m VERBOSE: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal update \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;241m-\u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msec\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[93], line 506\u001b[0m, in \u001b[0;36mTrainMNISTCluster.dec_param_update\u001b[0;34m(self, cluster_assign)\u001b[0m\n\u001b[1;32m    504\u001b[0m m_j_param \u001b[38;5;241m=\u001b[39m m_j_params[name]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    505\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m\n\u001b[0;32m--> 506\u001b[0m param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm_i_param\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm_j_param\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "config = get_config()\n",
        "\n",
        "config['train_seed'] = config['data_seed']\n",
        "\n",
        "print(\"config:\",config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "exp = TrainMNISTCluster(config, device)\n",
        "exp.setup()\n",
        "exp.run()\n",
        "duration = (time.time() - start_time)\n",
        "print(\"---train cluster Ended in %0.2f hour (%.3f sec) \" % (duration/float(3600), duration))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
