{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OHJWesKs-tqd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "import pickle\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "import random\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from util import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAghP_o0-tqe"
      },
      "source": [
        "Reads Config file and prepares the arguments you can choose in the config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BbUZJ2E--tqe"
      },
      "outputs": [],
      "source": [
        "LR_DECAY = False\n",
        "def get_config():\n",
        "\n",
        "    # read config json and update the sysarg\n",
        "    with open(\"config.json\", \"r\") as read_file:\n",
        "        config = json.load(read_file)\n",
        "\n",
        "    if config[\"config_override\"] == \"\":\n",
        "        del config['config_override']\n",
        "    else:\n",
        "        print(config['config_override'])\n",
        "        config_override = json.loads(config['config_override'])\n",
        "        del config['config_override']\n",
        "        config.update(config_override)\n",
        "\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-J1YEoM-tqe"
      },
      "source": [
        "Class SimpleLinear with simple MLP for MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "id5Wyt-V-tqf"
      },
      "outputs": [],
      "source": [
        "class SimpleLinear(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, h1=2048):\n",
        "        super().__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, h1)\n",
        "        self.fc2 = torch.nn.Linear(h1, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # def weight(self):\n",
        "    #     return self.linear1.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qFBH01M-tqf"
      },
      "source": [
        "Class TrainMNISTCluster with all the methods needed to run the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IkGiGQ2G-tqf"
      },
      "outputs": [],
      "source": [
        "class TrainMNISTCluster(object):\n",
        "    def __init__(self, config, device):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        assert self.config['m'] % self.config['p'] == 0\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
        "\n",
        "        self.result_fname = os.path.join(self.config['project_dir'], 'results.pickle')\n",
        "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint.pt')\n",
        "\n",
        "        self.setup_datasets()\n",
        "        self.setup_models()\n",
        "\n",
        "        self.epoch = None\n",
        "        self.lr = None\n",
        "\n",
        "\n",
        "    def setup_datasets(self):\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        # generate indices for each dataset\n",
        "        # also write cluster info\n",
        "\n",
        "        MNIST_TRAINSET_DATA_SIZE = 60000\n",
        "        MNIST_TESTSET_DATA_SIZE = 10000\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        self.dataset = {}\n",
        "\n",
        "        if cfg['uneven'] == True:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        else:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def _setup_dataset_random_n(self, num_data, p, m, n):\n",
        "\n",
        "        print(\"m:\",m)\n",
        "        print(\"p:\",p)\n",
        "        print(\"num_data:\",num_data)\n",
        "\n",
        "        dataset = {}\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        data_indices = []\n",
        "        cluster_assign = [[] for _ in range(m)]\n",
        "\n",
        "        m_per_cluster = m // p\n",
        "\n",
        "        for p_i in range(p):\n",
        "\n",
        "            ll = list(np.random.permutation(num_data))\n",
        "\n",
        "            ll2 = chunkify_uneven(ll, m_per_cluster) # splits ll into m lists\n",
        "            data_indices += ll2\n",
        "\n",
        "            for i in range(m_per_cluster):\n",
        "                cluster_assign[p_i * m_per_cluster + i].append(p_i)\n",
        "\n",
        "        for m_i in range(m):\n",
        "            p_i_ = cluster_assign[m_i]\n",
        "            for i in range(cfg['k\\'']):\n",
        "                if random.random() < 0.2:  # 20% chance\n",
        "                    if i + p_i_[0] > 3:\n",
        "                        cluster_assign[m_i].append(0)\n",
        "                    else:\n",
        "                        cluster_assign[m_i].append(p_i_[0] + i)\n",
        "\n",
        "        print(\"penis\")\n",
        "        data_indices = np.array(data_indices, dtype=object)\n",
        "        # cluster_assign = np.array(cluster_assign, dtype=object)\n",
        "        #assert data_indices.shape[0] == cluster_assign.shape[0]\n",
        "        # assert data_indices.shape[0] == m\n",
        "\n",
        "\n",
        "        return data_indices, cluster_assign\n",
        "\n",
        "\n",
        "    def _load_MNIST(self, train=True):\n",
        "        transforms = torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               # torchvision.transforms.Normalize(\n",
        "                               #   (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "        if train:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
        "        else:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
        "\n",
        "        dl = DataLoader(mnist_dataset)\n",
        "\n",
        "        X = dl.dataset.data # (60000,28, 28)\n",
        "        y = dl.dataset.targets #(60000)\n",
        "\n",
        "        # normalize to have 0 ~ 1 range in each pixel\n",
        "\n",
        "        X = X / 255.0\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    # Need p models for each client\n",
        "\n",
        "    def setup_models(self):\n",
        "        np.random.seed(self.config['train_seed'])\n",
        "        torch.manual_seed(self.config['train_seed'])\n",
        "\n",
        "        p = self.config['p']\n",
        "\n",
        "        self.models = [ SimpleLinear(h1 = self.config['h1']).to(self.device) for p_i in range(p)] # p models with p different params of dimension(1,d)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        num_epochs = self.config['num_epochs']\n",
        "        lr = self.config['lr']\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # epoch -1\n",
        "        self.epoch = -1\n",
        "\n",
        "        result = {}\n",
        "        result['epoch'] = -1\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=True)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['train'] = res\n",
        "\n",
        "        self.print_epoch_stats(res)\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=False)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['test'] = res\n",
        "        self.print_epoch_stats(res)\n",
        "        results.append(result)\n",
        "\n",
        "        # this will be used in next epoch\n",
        "        cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.epoch = epoch\n",
        "\n",
        "            result = {}\n",
        "            result['epoch'] = epoch\n",
        "\n",
        "            lr = self.lr_schedule(epoch)\n",
        "            result['lr'] = lr\n",
        "\n",
        "            t0 = time.time()\n",
        "            result['train'] = self.train(cluster_assign, lr = lr)\n",
        "            t1 = time.time()\n",
        "            train_time = t1-t0\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=True)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            res['train_time'] = train_time\n",
        "            res['lr'] = lr\n",
        "            result['train'] = res\n",
        "\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=False)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            result['test'] = res\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "            # this will be used in next epoch's gradient update\n",
        "            cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
        "                with open(self.result_fname, 'wb') as outfile:\n",
        "                    pickle.dump(results, outfile)\n",
        "                    print(f'result written at {self.result_fname}')\n",
        "                self.save_checkpoint()\n",
        "                print(f'checkpoint written at {self.checkpoint_fname}')\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['train']['loss'] for r in results], label='train')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('loss')\n",
        "        plt.title('Training Loss per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'train_loss.png'))\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['test']['acc'] for r in results], label='train')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('test accuracy')\n",
        "        plt.title('Test Accuracy per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'test_acc.png'))\n",
        "\n",
        "\n",
        "    def lr_schedule(self, epoch):\n",
        "        if self.lr is None:\n",
        "            self.lr = self.config['lr']\n",
        "\n",
        "        if epoch % 50 == 0 and epoch != 0 and LR_DECAY:\n",
        "            self.lr = self.lr * 0.1\n",
        "\n",
        "        return self.lr\n",
        "\n",
        "\n",
        "    def print_epoch_stats(self, res):\n",
        "        if res['is_train']:\n",
        "            data_str = 'tr'\n",
        "        else:\n",
        "            data_str = 'tst'\n",
        "\n",
        "        if 'train_time' in res:\n",
        "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
        "        else:\n",
        "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
        "\n",
        "        if 'lr' in res:\n",
        "            lr_str = f\" lr {res['lr']:4f}\"\n",
        "        else:\n",
        "            lr_str = \"\"\n",
        "\n",
        "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} clct{res['cl_ct']}{lr_str} {time_str}\"\n",
        "\n",
        "        print(str0)\n",
        "\n",
        "    def train(self, cluster_assign, lr):\n",
        "        VERBOSE = 0\n",
        "\n",
        "        cfg = self.config\n",
        "        m = cfg['m']\n",
        "        p = cfg['p']\n",
        "        tau = cfg['tau']\n",
        "\n",
        "        # run local update\n",
        "        t0 = time.time()\n",
        "\n",
        "\n",
        "        updated_models = [[None for _ in range(p)] for _ in range(m)]\n",
        "        for m_i in range(m):\n",
        "            if VERBOSE and m_i % 100 == 0: print(f'm {m_i}/{m} processing \\r', end ='')\n",
        "\n",
        "            (X, y) = self.load_data(m_i)\n",
        "\n",
        "            for p_i in cluster_assign[m_i]:\n",
        "\n",
        "                model = copy.deepcopy(self.models[p_i])\n",
        "\n",
        "                # LOCAL UPDATE PER MACHINE tau times\n",
        "                for step_i in range(tau):\n",
        "\n",
        "                    y_logit = model(X)\n",
        "                    loss = self.criterion(y_logit, y)\n",
        "\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.local_param_update(model, lr)\n",
        "\n",
        "                model.zero_grad()\n",
        "\n",
        "                updated_models[m_i][p_i] = model\n",
        "\n",
        "        t02 = time.time()\n",
        "        # print(f'running single ..took {t02-t01:.3f}sec')\n",
        "\n",
        "\n",
        "        t1 = time.time()\n",
        "        if VERBOSE: print(f'local update {t1-t0:.3f}sec')\n",
        "\n",
        "        # apply gradient update\n",
        "        t0 = time.time()\n",
        "\n",
        "        # CLUSTER MACHINES INTO p_i's\n",
        "        local_models = [[] for p_i in range(p)]\n",
        "        for m_i in range(m):\n",
        "            for p_i in cluster_assign[m_i]:\n",
        "                local_models[p_i].append(updated_models[m_i][p_i])\n",
        "\n",
        "        # NEEDS TO BE DECENTRALIZED\n",
        "        for p_i, models in enumerate(local_models):\n",
        "            if len(models) > 0:\n",
        "                self.global_param_update(models, self.models[p_i])\n",
        "        t1 = time.time()\n",
        "\n",
        "        if VERBOSE: print(f'global update {t1-t0:.3f}sec')\n",
        "\n",
        "    def check_local_model_loss(self, local_models):\n",
        "        # for debugging\n",
        "        m = self.config['m']\n",
        "\n",
        "        losses = []\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i)\n",
        "            y_logit = local_models[m_i](X)\n",
        "            loss = self.criterion(y_logit, y)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        return np.array(losses)\n",
        "\n",
        "\n",
        "    def get_inference_stats(self, train = True):\n",
        "        cfg = self.config\n",
        "        if train:\n",
        "            m = cfg['m']\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            m = cfg['m_test']\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        p = cfg['p']\n",
        "\n",
        "\n",
        "        num_data = 0\n",
        "        losses = {}\n",
        "        corrects = {}\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i, train=train) # load batch data rotated\n",
        "\n",
        "            for p_i in range(p):\n",
        "                y_logit = self.models[p_i](X)\n",
        "                loss = self.criterion(y_logit, y) # loss of\n",
        "                n_correct = self.n_correct(y_logit, y)\n",
        "\n",
        "                # if torch.isnan(loss):\n",
        "                #     print(\"nan loss: \", dataset['data_indices'][m_i])\n",
        "\n",
        "                losses[(m_i,p_i)] = loss.item()\n",
        "                corrects[(m_i,p_i)] = n_correct\n",
        "\n",
        "                num_data += X.shape[0]\n",
        "\n",
        "        # calculate loss and cluster the machines\n",
        "        cluster_assign = [[] for _ in range(m)]\n",
        "        for m_i in range(m):\n",
        "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
        "            min_p_i = np.argmin(machine_losses)\n",
        "            cp_machine_losses = copy.deepcopy(machine_losses)\n",
        "            cp_machine_losses.pop(min_p_i)\n",
        "            sec_min_p_i = np.argmin(cp_machine_losses)\n",
        "            rho = (machine_losses[min_p_i] + 0.2*(abs(machine_losses[min_p_i] - machine_losses[sec_min_p_i])))\n",
        "            cnt = 0\n",
        "            assigned_clusters = set()\n",
        "            while cnt < cfg[\"k'\"]:\n",
        "                for p_i in range(p):\n",
        "                    if machine_losses[p_i] <= rho and p_i not in assigned_clusters:\n",
        "                        cluster_assign[m_i].append(p_i)\n",
        "                        assigned_clusters.add(p_i)\n",
        "                        break\n",
        "                cnt += 1\n",
        "\n",
        "        # calculate optimal model's loss, acc over all models\n",
        "        min_corrects = []\n",
        "        min_losses = []\n",
        "        for m_i, p_i_list in enumerate(cluster_assign):\n",
        "            for p_i in p_i_list:\n",
        "                min_loss = losses[(m_i, p_i)]\n",
        "                min_losses.append(min_loss)\n",
        "\n",
        "                min_correct = corrects[(m_i, p_i)]\n",
        "                min_corrects.append(min_correct)\n",
        "\n",
        "        # print(\"losses: \", min_losses)\n",
        "        loss = np.mean(min_losses)\n",
        "        acc = np.sum(min_corrects) / num_data\n",
        "\n",
        "\n",
        "        # check cluster assignment acc\n",
        "        cl_acc = np.mean([set(cluster_assign[m_i]) == set(dataset['cluster_assign'][m_i]) for m_i in range(m)])\n",
        "        cl_ct = [np.sum([p_i in cluster_assign[m_i] for m_i in range(m)]) for p_i in range(p)]\n",
        "\n",
        "        res = {} # results\n",
        "        # res['losses'] = losses\n",
        "        # res['corrects'] = corrects\n",
        "        res['cluster_assign'] = cluster_assign\n",
        "        res['num_data'] = num_data\n",
        "        res['loss'] = loss\n",
        "        res['acc'] = acc\n",
        "        res['cl_acc'] = cl_acc\n",
        "        res['cl_ct'] = cl_ct\n",
        "        res['is_train'] = train\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        return res\n",
        "\n",
        "    def n_correct(self, y_logit, y):\n",
        "        _, predicted = torch.max(y_logit.data, 1)\n",
        "        correct = (predicted == y).sum().item()\n",
        "\n",
        "        return correct\n",
        "\n",
        "    # TODO Does every Cluster get 4 clients with the same data, but rotated differently?\n",
        "\n",
        "    def load_data(self, m_i, train=True):\n",
        "        # this part is very fast since its just rearranging models\n",
        "        cfg = self.config\n",
        "\n",
        "        if train:\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        indices = dataset['data_indices'][m_i]\n",
        "        p_i = dataset['cluster_assign'][m_i]\n",
        "\n",
        "        X_batch = dataset['X'][indices]\n",
        "        y_batch = dataset['y'][indices]\n",
        "\n",
        "        # k : how many times rotate 90 degree\n",
        "        # k =1 : 90 , k=2 180, k=3 270\n",
        "        if cfg['p'] == 4:\n",
        "            k = p_i[0]\n",
        "        elif cfg['p'] == 2:\n",
        "            k = (p_i[0] % 2) * 2\n",
        "        elif cfg['p'] == 1:\n",
        "            k = 0\n",
        "        else:\n",
        "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
        "\n",
        "        X_batch2 = torch.rot90(X_batch, k=int(k), dims=(1, 2))\n",
        "        X_batch3 = X_batch2.reshape(-1, 28 * 28)\n",
        "\n",
        "        if len(p_i) > 1:\n",
        "            additional_X_batches = []\n",
        "            additional_y_batches = []       \n",
        "\n",
        "            for i in range(1, len(p_i)-1):\n",
        "                additional_rotation = (p_i[0] + i) % 4\n",
        "                X_batch_additional_rot = torch.rot90(X_batch, k=additional_rotation, dims=(1, 2))\n",
        "                X_batch_additional_rot = X_batch_additional_rot.reshape(-1, 28 * 28)\n",
        "                additional_X_batches.append(X_batch_additional_rot)\n",
        "                additional_y_batches.append(y_batch)\n",
        "\n",
        "            X_batch3 = torch.cat([X_batch3] + additional_X_batches, dim=0)\n",
        "            y_batch = torch.cat([y_batch] + additional_y_batches, dim=0)\n",
        "\n",
        "        return X_batch3, y_batch\n",
        "\n",
        "\n",
        "    def local_param_update(self, model, lr):\n",
        "\n",
        "        # gradient update manually\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data -= lr * param.grad\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace() # we need to check the output of name, check if duplicate exists\n",
        "    \n",
        "\n",
        "    def global_param_update(self, local_models, global_model):\n",
        "\n",
        "        # average of each weight\n",
        "\n",
        "        weights = {}\n",
        "\n",
        "        for m_i, local_model in enumerate(local_models):\n",
        "            for name, param in local_model.named_parameters():\n",
        "                if name not in weights:\n",
        "                    weights[name] = torch.zeros_like(param.data)\n",
        "\n",
        "                weights[name] += param.data\n",
        "\n",
        "        for name, param in global_model.named_parameters():\n",
        "            weights[name] /= len(local_models)\n",
        "            param.data = weights[name]\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def dec_param_update(self, local_models, global_model):\n",
        "\n",
        "        num_clients = len(local_models)\n",
        "\n",
        "        if num_clients == 0:\n",
        "            return\n",
        "\n",
        "        if num_clients == 1:\n",
        "            bc_client = dict(local_models[0].named_parameters())\n",
        "            for name, param in global_model.named_parameters():\n",
        "                param.data = bc_client[name].data.clone()\n",
        "            return\n",
        "\n",
        "        max_e = 100\n",
        "        if num_clients <= max_e:\n",
        "            e = num_clients - 1\n",
        "        else:\n",
        "            e = min(max_e, int(np.log(num_clients) * 10))\n",
        "\n",
        "        if e >= num_clients:\n",
        "            e = num_clients - 1\n",
        "\n",
        "        client_indices = list(range(num_clients))\n",
        "\n",
        "        for m_i, local_model in enumerate(local_models):\n",
        "            selected_clients = random.sample([i for i in client_indices if i != m_i], e)\n",
        "\n",
        "            for m_j in selected_clients:\n",
        "\n",
        "                m_j_params = dict(local_models[m_j].named_parameters())\n",
        "\n",
        "                for name, param in local_model.named_parameters():\n",
        "                    m_i_param = param.data.clone()\n",
        "                    m_j_param = m_j_params[name].data.clone()\n",
        "                    param.data = (m_i_param + m_j_param) / 2\n",
        "\n",
        "        bc_client = random.choice(client_indices)\n",
        "        bc_client_params = dict(local_models[bc_client].named_parameters())\n",
        "        for name, param in global_model.named_parameters():\n",
        "            param.data = bc_client_params[name].data.clone()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def test(self, train=False):\n",
        "        return self.get_inference_stats(train=train)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        models_to_save = [model.state_dict() for model in self.models]\n",
        "        torch.save({'models':models_to_save}, self.checkpoint_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADsUSUi-tqf"
      },
      "source": [
        "Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T_XDv25r-tqf",
        "outputId": "9c8f4300-c792-4e49-be40-c694fa066e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config: {'m': 1200, 'm_test': 200, 'p': 4, 'n': 200, 'uneven': True, \"k'\": 2, 'h1': 200, 'num_epochs': 300, 'batch_size': 100, 'tau': 10, 'lr': 0.1, 'data_seed': 10, 'train_seed': 10, 'project_dir': 'output'}\n",
            "Using device: cuda\n",
            "m: 1200\n",
            "p: 4\n",
            "num_data: 60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "penis\n",
            "m: 200\n",
            "p: 4\n",
            "num_data: 10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "penis\n",
            "Epoch -1 tr: l 2.294 a 0.029 clct[349, 317, 168, 436] 1.792sec\n",
            "Epoch -1 tst: l 2.292 a 0.030 clct[56, 68, 23, 68] 0.266sec\n",
            "Epoch 0 tr: l 2.176 a 0.094 clct[313, 458, 129, 323] lr 0.100000 5.449sec(train) 1.576sec(infer)\n",
            "Epoch 0 tst: l 2.173 a 0.095 clct[52, 78, 17, 57] 0.257sec\n",
            "result written at output\\results.pickle\n",
            "checkpoint written at output\\checkpoint.pt\n",
            "Epoch 1 tr: l 1.936 a 0.154 clct[310, 294, 303, 306] lr 0.100000 4.952sec(train) 1.456sec(infer)\n",
            "Epoch 1 tst: l 1.929 a 0.154 clct[53, 51, 47, 50] 0.259sec\n",
            "Epoch 2 tr: l 1.608 a 0.178 clct[304, 295, 309, 300] lr 0.100000 4.998sec(train) 1.457sec(infer)\n",
            "Epoch 2 tst: l 1.596 a 0.178 clct[53, 50, 48, 49] 0.243sec\n",
            "Epoch 3 tr: l 1.305 a 0.189 clct[302, 295, 308, 301] lr 0.100000 5.355sec(train) 1.501sec(infer)\n",
            "Epoch 3 tst: l 1.289 a 0.189 clct[53, 50, 48, 49] 0.248sec\n",
            "Epoch 4 tr: l 1.080 a 0.196 clct[302, 295, 305, 304] lr 0.100000 5.001sec(train) 1.454sec(infer)\n",
            "Epoch 4 tst: l 1.061 a 0.196 clct[53, 50, 48, 49] 0.276sec\n",
            "Epoch 5 tr: l 0.925 a 0.200 clct[302, 295, 300, 309] lr 0.100000 5.204sec(train) 1.444sec(infer)\n",
            "Epoch 5 tst: l 0.906 a 0.200 clct[53, 50, 48, 49] 0.248sec\n",
            "Epoch 6 tr: l 0.819 a 0.203 clct[301, 296, 298, 311] lr 0.100000 5.104sec(train) 1.394sec(infer)\n",
            "Epoch 6 tst: l 0.799 a 0.203 clct[53, 50, 47, 50] 0.231sec\n",
            "Epoch 7 tr: l 0.743 a 0.206 clct[301, 296, 295, 314] lr 0.100000 4.673sec(train) 1.529sec(infer)\n",
            "Epoch 7 tst: l 0.724 a 0.205 clct[53, 50, 47, 50] 0.257sec\n",
            "Epoch 8 tr: l 0.687 a 0.207 clct[300, 295, 296, 314] lr 0.100000 5.304sec(train) 1.556sec(infer)\n",
            "Epoch 8 tst: l 0.669 a 0.207 clct[53, 50, 47, 50] 0.261sec\n",
            "Epoch 9 tr: l 0.643 a 0.208 clct[299, 295, 296, 314] lr 0.100000 5.152sec(train) 1.579sec(infer)\n",
            "Epoch 9 tst: l 0.627 a 0.208 clct[53, 50, 47, 50] 0.269sec\n",
            "Epoch 10 tr: l 0.609 a 0.210 clct[299, 295, 296, 314] lr 0.100000 4.993sec(train) 1.554sec(infer)\n",
            "Epoch 10 tst: l 0.593 a 0.209 clct[53, 50, 47, 50] 0.267sec\n",
            "result written at output\\results.pickle\n",
            "checkpoint written at output\\checkpoint.pt\n",
            "Epoch 11 tr: l 0.581 a 0.210 clct[298, 295, 296, 314] lr 0.100000 5.228sec(train) 1.569sec(infer)\n",
            "Epoch 11 tst: l 0.567 a 0.210 clct[53, 50, 47, 50] 0.261sec\n",
            "Epoch 12 tr: l 0.558 a 0.211 clct[297, 295, 296, 314] lr 0.100000 4.949sec(train) 1.534sec(infer)\n",
            "Epoch 12 tst: l 0.545 a 0.211 clct[53, 50, 47, 50] 0.254sec\n",
            "Epoch 13 tr: l 0.539 a 0.212 clct[296, 295, 296, 314] lr 0.100000 4.916sec(train) 1.460sec(infer)\n",
            "Epoch 13 tst: l 0.527 a 0.212 clct[53, 50, 47, 50] 0.250sec\n",
            "Epoch 14 tr: l 0.522 a 0.212 clct[295, 295, 296, 314] lr 0.100000 4.918sec(train) 1.472sec(infer)\n",
            "Epoch 14 tst: l 0.512 a 0.212 clct[53, 50, 47, 50] 0.243sec\n",
            "Epoch 15 tr: l 0.508 a 0.213 clct[295, 295, 296, 314] lr 0.100000 5.333sec(train) 1.533sec(infer)\n",
            "Epoch 15 tst: l 0.499 a 0.213 clct[53, 50, 47, 50] 0.260sec\n",
            "Epoch 16 tr: l 0.496 a 0.213 clct[295, 295, 296, 314] lr 0.100000 5.388sec(train) 1.603sec(infer)\n",
            "Epoch 16 tst: l 0.488 a 0.213 clct[53, 50, 47, 50] 0.271sec\n",
            "Epoch 17 tr: l 0.486 a 0.214 clct[295, 295, 296, 314] lr 0.100000 5.374sec(train) 1.599sec(infer)\n",
            "Epoch 17 tst: l 0.478 a 0.214 clct[53, 50, 47, 50] 0.263sec\n",
            "Epoch 18 tr: l 0.476 a 0.214 clct[295, 295, 295, 315] lr 0.100000 5.418sec(train) 1.513sec(infer)\n",
            "Epoch 18 tst: l 0.469 a 0.214 clct[53, 50, 47, 50] 0.253sec\n",
            "Epoch 19 tr: l 0.468 a 0.214 clct[295, 295, 295, 315] lr 0.100000 4.939sec(train) 1.486sec(infer)\n",
            "Epoch 19 tst: l 0.461 a 0.215 clct[53, 50, 47, 50] 0.244sec\n",
            "Epoch 20 tr: l 0.461 a 0.215 clct[295, 295, 295, 315] lr 0.100000 4.988sec(train) 1.481sec(infer)\n",
            "Epoch 20 tst: l 0.454 a 0.215 clct[53, 50, 47, 50] 0.250sec\n",
            "result written at output\\results.pickle\n",
            "checkpoint written at output\\checkpoint.pt\n",
            "Epoch 21 tr: l 0.454 a 0.215 clct[295, 295, 295, 315] lr 0.100000 4.877sec(train) 1.467sec(infer)\n",
            "Epoch 21 tst: l 0.448 a 0.215 clct[53, 50, 47, 50] 0.255sec\n",
            "Epoch 22 tr: l 0.448 a 0.215 clct[295, 295, 295, 315] lr 0.100000 4.893sec(train) 1.532sec(infer)\n",
            "Epoch 22 tst: l 0.443 a 0.215 clct[53, 50, 47, 50] 0.251sec\n",
            "Epoch 23 tr: l 0.442 a 0.216 clct[295, 295, 295, 315] lr 0.100000 4.909sec(train) 1.510sec(infer)\n",
            "Epoch 23 tst: l 0.437 a 0.216 clct[53, 50, 47, 50] 0.264sec\n",
            "Epoch 24 tr: l 0.437 a 0.216 clct[295, 295, 295, 315] lr 0.100000 4.996sec(train) 1.494sec(infer)\n",
            "Epoch 24 tst: l 0.433 a 0.216 clct[53, 50, 47, 50] 0.255sec\n",
            "Epoch 25 tr: l 0.432 a 0.216 clct[295, 295, 295, 315] lr 0.100000 5.016sec(train) 1.479sec(infer)\n",
            "Epoch 25 tst: l 0.428 a 0.216 clct[53, 50, 47, 50] 0.247sec\n",
            "Epoch 26 tr: l 0.427 a 0.217 clct[295, 295, 295, 315] lr 0.100000 4.972sec(train) 1.625sec(infer)\n",
            "Epoch 26 tst: l 0.424 a 0.216 clct[53, 50, 47, 50] 0.267sec\n",
            "Epoch 27 tr: l 0.423 a 0.217 clct[295, 295, 295, 315] lr 0.100000 5.081sec(train) 1.477sec(infer)\n",
            "Epoch 27 tst: l 0.421 a 0.217 clct[53, 50, 47, 50] 0.259sec\n",
            "Epoch 28 tr: l 0.419 a 0.217 clct[295, 295, 295, 315] lr 0.100000 4.968sec(train) 1.482sec(infer)\n",
            "Epoch 28 tst: l 0.417 a 0.217 clct[53, 50, 47, 50] 0.264sec\n",
            "Epoch 29 tr: l 0.415 a 0.217 clct[295, 295, 295, 315] lr 0.100000 5.067sec(train) 1.515sec(infer)\n",
            "Epoch 29 tst: l 0.414 a 0.217 clct[53, 50, 47, 50] 0.261sec\n",
            "Epoch 30 tr: l 0.412 a 0.217 clct[295, 295, 295, 315] lr 0.100000 5.026sec(train) 1.503sec(infer)\n",
            "Epoch 30 tst: l 0.411 a 0.217 clct[53, 50, 47, 50] 0.255sec\n",
            "result written at output\\results.pickle\n",
            "checkpoint written at output\\checkpoint.pt\n",
            "Epoch 31 tr: l 0.408 a 0.218 clct[295, 295, 295, 315] lr 0.100000 4.943sec(train) 1.499sec(infer)\n",
            "Epoch 31 tst: l 0.408 a 0.217 clct[53, 50, 47, 50] 0.258sec\n",
            "Epoch 32 tr: l 0.405 a 0.218 clct[295, 295, 295, 315] lr 0.100000 4.990sec(train) 1.506sec(infer)\n",
            "Epoch 32 tst: l 0.405 a 0.217 clct[53, 50, 47, 50] 0.254sec\n",
            "Epoch 33 tr: l 0.403 a 0.218 clct[295, 295, 295, 316] lr 0.100000 4.913sec(train) 1.510sec(infer)\n",
            "Epoch 33 tst: l 0.402 a 0.218 clct[53, 50, 47, 50] 0.251sec\n",
            "Epoch 34 tr: l 0.400 a 0.218 clct[295, 295, 295, 316] lr 0.100000 4.993sec(train) 1.529sec(infer)\n",
            "Epoch 34 tst: l 0.400 a 0.218 clct[53, 50, 47, 50] 0.257sec\n",
            "Epoch 35 tr: l 0.397 a 0.218 clct[295, 295, 295, 316] lr 0.100000 5.083sec(train) 1.510sec(infer)\n",
            "Epoch 35 tst: l 0.397 a 0.218 clct[53, 50, 47, 50] 0.255sec\n",
            "Epoch 36 tr: l 0.394 a 0.219 clct[295, 295, 295, 316] lr 0.100000 4.930sec(train) 1.520sec(infer)\n",
            "Epoch 36 tst: l 0.395 a 0.218 clct[53, 50, 47, 50] 0.257sec\n",
            "Epoch 37 tr: l 0.391 a 0.219 clct[295, 295, 295, 316] lr 0.100000 5.090sec(train) 1.486sec(infer)\n",
            "Epoch 37 tst: l 0.393 a 0.218 clct[53, 50, 47, 50] 0.251sec\n",
            "Epoch 38 tr: l 0.389 a 0.219 clct[295, 295, 295, 316] lr 0.100000 4.870sec(train) 1.499sec(infer)\n",
            "Epoch 38 tst: l 0.390 a 0.218 clct[53, 50, 47, 50] 0.246sec\n",
            "Epoch 39 tr: l 0.386 a 0.219 clct[295, 295, 295, 316] lr 0.100000 4.947sec(train) 1.492sec(infer)\n",
            "Epoch 39 tst: l 0.388 a 0.218 clct[53, 50, 47, 50] 0.249sec\n",
            "Epoch 40 tr: l 0.384 a 0.219 clct[295, 295, 295, 316] lr 0.100000 4.914sec(train) 1.484sec(infer)\n",
            "Epoch 40 tst: l 0.386 a 0.219 clct[53, 50, 47, 50] 0.247sec\n",
            "result written at output\\results.pickle\n",
            "checkpoint written at output\\checkpoint.pt\n",
            "Epoch 41 tr: l 0.382 a 0.219 clct[295, 295, 295, 316] lr 0.100000 4.920sec(train) 1.475sec(infer)\n",
            "Epoch 41 tst: l 0.384 a 0.219 clct[53, 50, 47, 50] 0.245sec\n",
            "Epoch 42 tr: l 0.379 a 0.220 clct[295, 295, 295, 316] lr 0.100000 5.013sec(train) 1.493sec(infer)\n",
            "Epoch 42 tst: l 0.382 a 0.219 clct[53, 50, 47, 50] 0.257sec\n",
            "Epoch 43 tr: l 0.377 a 0.220 clct[295, 295, 295, 316] lr 0.100000 4.842sec(train) 1.482sec(infer)\n",
            "Epoch 43 tst: l 0.381 a 0.219 clct[53, 50, 47, 50] 0.252sec\n",
            "Epoch 44 tr: l 0.375 a 0.220 clct[295, 295, 295, 316] lr 0.100000 4.932sec(train) 1.475sec(infer)\n",
            "Epoch 44 tst: l 0.379 a 0.219 clct[53, 50, 47, 50] 0.261sec\n",
            "Epoch 45 tr: l 0.373 a 0.220 clct[295, 295, 295, 316] lr 0.100000 4.962sec(train) 1.479sec(infer)\n",
            "Epoch 45 tst: l 0.377 a 0.219 clct[53, 50, 47, 50] 0.248sec\n",
            "Epoch 46 tr: l 0.371 a 0.220 clct[295, 295, 295, 316] lr 0.100000 4.868sec(train) 1.487sec(infer)\n",
            "Epoch 46 tst: l 0.375 a 0.219 clct[53, 50, 47, 50] 0.247sec\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainMNISTCluster(config, device)\n\u001b[0;32m     11\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m duration \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---train cluster Ended in \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;124m hour (\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m sec) \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (duration\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m3600\u001b[39m), duration))\n",
            "Cell \u001b[1;32mIn[4], line 200\u001b[0m, in \u001b[0;36mTrainMNISTCluster.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m    199\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 200\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_assign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    202\u001b[0m train_time \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
            "Cell \u001b[1;32mIn[4], line 308\u001b[0m, in \u001b[0;36mTrainMNISTCluster.train\u001b[1;34m(self, cluster_assign, lr)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# LOCAL UPDATE PER MACHINE tau times\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tau):\n\u001b[1;32m--> 308\u001b[0m     y_logit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(y_logit, y)\n\u001b[0;32m    311\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32me:\\anaconda\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\anaconda\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mSimpleLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# x = F.sigmoid(self.fc1(x))\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32me:\\anaconda\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\anaconda\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32me:\\anaconda\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "config = get_config()\n",
        "\n",
        "config['train_seed'] = config['data_seed']\n",
        "\n",
        "print(\"config:\",config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "exp = TrainMNISTCluster(config, device)\n",
        "exp.setup()\n",
        "exp.run()\n",
        "duration = (time.time() - start_time)\n",
        "print(\"---train cluster Ended in %0.2f hour (%.3f sec) \" % (duration/float(3600), duration))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
