{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "OHJWesKs-tqd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "import pickle\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "import random\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from util import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAghP_o0-tqe"
      },
      "source": [
        "Reads Config file and prepares the arguments you can choose in the config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "BbUZJ2E--tqe"
      },
      "outputs": [],
      "source": [
        "LR_DECAY = False\n",
        "def get_config():\n",
        "\n",
        "    # read config json and update the sysarg\n",
        "    with open(\"config.json\", \"r\") as read_file:\n",
        "        config = json.load(read_file)\n",
        "\n",
        "    if config[\"config_override\"] == \"\":\n",
        "        del config['config_override']\n",
        "    else:\n",
        "        print(config['config_override'])\n",
        "        config_override = json.loads(config['config_override'])\n",
        "        del config['config_override']\n",
        "        config.update(config_override)\n",
        "\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-J1YEoM-tqe"
      },
      "source": [
        "Class SimpleLinear with simple MLP for MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "id5Wyt-V-tqf"
      },
      "outputs": [],
      "source": [
        "class SimpleLinear(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, h1=2048):\n",
        "        super().__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, h1)\n",
        "        self.fc2 = torch.nn.Linear(h1, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # def weight(self):\n",
        "    #     return self.linear1.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qFBH01M-tqf"
      },
      "source": [
        "Class TrainMNISTCluster with all the methods needed to run the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "IkGiGQ2G-tqf"
      },
      "outputs": [],
      "source": [
        "class TrainMNISTCluster(object):\n",
        "    def __init__(self, config, device):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        assert self.config['m'] % self.config['p'] == 0\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        os.makedirs(self.config['project_dir'], exist_ok = True)\n",
        "\n",
        "        self.result_fname = os.path.join(self.config['project_dir'], 'results.pickle')\n",
        "        self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint.pt')\n",
        "\n",
        "        self.setup_datasets()\n",
        "        self.setup_models()\n",
        "\n",
        "        self.epoch = None\n",
        "        self.lr = None\n",
        "\n",
        "\n",
        "    def setup_datasets(self):\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        # generate indices for each dataset\n",
        "        # also write cluster info\n",
        "\n",
        "        MNIST_TRAINSET_DATA_SIZE = 60000\n",
        "        MNIST_TESTSET_DATA_SIZE = 10000\n",
        "\n",
        "        np.random.seed(self.config['data_seed'])\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        self.dataset = {}\n",
        "\n",
        "        if cfg['uneven'] == True:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['data_cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            dataset['cluster_assign'] = dataset['data_cluster_assign']\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['data_cluster_assign'] = \\\n",
        "                self._setup_dataset_random_n(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            dataset['cluster_assign'] = dataset['data_cluster_assign']\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        else:\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['data_cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TRAINSET_DATA_SIZE, cfg['p'], cfg['m'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=True)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            dataset['cluster_assign'] = dataset['data_cluster_assign']\n",
        "            self.dataset['train'] = dataset\n",
        "\n",
        "            dataset = {}\n",
        "            dataset['data_indices'], dataset['data_cluster_assign'] = \\\n",
        "                self._setup_dataset(MNIST_TESTSET_DATA_SIZE, cfg['p'], cfg['m_test'], cfg['n'])\n",
        "            (X, y) = self._load_MNIST(train=False)\n",
        "            dataset['X'] = X\n",
        "            dataset['y'] = y\n",
        "            dataset['cluster_assign'] = dataset['data_cluster_assign']\n",
        "            self.dataset['test'] = dataset\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def _setup_dataset_random_n(self, num_data, p, m, n):\n",
        "\n",
        "        print(\"m:\",m)\n",
        "        print(\"p:\",p)\n",
        "        print(\"num_data:\",num_data)\n",
        "\n",
        "        dataset = {}\n",
        "\n",
        "        cfg = self.config\n",
        "\n",
        "        data_indices = []\n",
        "        cluster_assign = [[] for _ in range(m)]\n",
        "\n",
        "        m_per_cluster = m // p\n",
        "\n",
        "        for p_i in range(p):\n",
        "\n",
        "            ll = list(np.random.permutation(num_data))\n",
        "\n",
        "            ll2 = chunkify_uneven(ll, m_per_cluster) # splits ll into m lists\n",
        "            data_indices += ll2\n",
        "\n",
        "            for i in range(m_per_cluster):\n",
        "                cluster_assign[p_i * m_per_cluster + i].append(p_i)\n",
        "\n",
        "        for m_i in range(m):\n",
        "            p_i_ = cluster_assign[m_i]\n",
        "            for i in range(cfg['k\\'']):\n",
        "                if random.random() < 0.2:  # 20% chance\n",
        "                    if i + p_i_[0] > 3:\n",
        "                        cluster_assign[m_i].append(0)\n",
        "                    else:\n",
        "                        cluster_assign[m_i].append(p_i_[0] + i)\n",
        "\n",
        "        data_indices = np.array(data_indices, dtype=object)\n",
        "        # cluster_assign = np.array(cluster_assign, dtype=object)\n",
        "        #assert data_indices.shape[0] == cluster_assign.shape[0]\n",
        "        # assert data_indices.shape[0] == m\n",
        "\n",
        "\n",
        "        return data_indices, cluster_assign\n",
        "\n",
        "\n",
        "    def _load_MNIST(self, train=True):\n",
        "        transforms = torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               # torchvision.transforms.Normalize(\n",
        "                               #   (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "        if train:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
        "        else:\n",
        "            mnist_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
        "\n",
        "        dl = DataLoader(mnist_dataset)\n",
        "\n",
        "        X = dl.dataset.data # (60000,28, 28)\n",
        "        y = dl.dataset.targets #(60000)\n",
        "\n",
        "        # normalize to have 0 ~ 1 range in each pixel\n",
        "\n",
        "        X = X / 255.0\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    # Need p models for each client\n",
        "\n",
        "    def setup_models(self):\n",
        "        np.random.seed(self.config['train_seed'])\n",
        "        torch.manual_seed(self.config['train_seed'])\n",
        "\n",
        "        p = self.config['p']\n",
        "\n",
        "        self.models = [ SimpleLinear(h1 = self.config['h1']).to(self.device) for p_i in range(p)] # p models with p different params of dimension(1,d)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        num_epochs = self.config['num_epochs']\n",
        "        lr = self.config['lr']\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # epoch -1\n",
        "        self.epoch = -1\n",
        "\n",
        "        result = {}\n",
        "        result['epoch'] = -1\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=True)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['train'] = res\n",
        "\n",
        "        self.print_epoch_stats(res)\n",
        "\n",
        "        t0 = time.time()\n",
        "        res = self.test(train=False)\n",
        "        t1 = time.time()\n",
        "        res['infer_time'] = t1-t0\n",
        "        result['test'] = res\n",
        "        self.print_epoch_stats(res)\n",
        "        results.append(result)\n",
        "\n",
        "        # this will be used in next epoch\n",
        "        cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.epoch = epoch\n",
        "\n",
        "            result = {}\n",
        "            result['epoch'] = epoch\n",
        "\n",
        "            lr = self.lr_schedule(epoch)\n",
        "            result['lr'] = lr\n",
        "\n",
        "            t0 = time.time()\n",
        "            result['train'] = self.train(cluster_assign, lr = lr)\n",
        "            t1 = time.time()\n",
        "            train_time = t1-t0\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=True)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            res['train_time'] = train_time\n",
        "            res['lr'] = lr\n",
        "            result['train'] = res\n",
        "\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            t0 = time.time()\n",
        "            res = self.test(train=False)\n",
        "            t1 = time.time()\n",
        "            res['infer_time'] = t1-t0\n",
        "            result['test'] = res\n",
        "            self.print_epoch_stats(res)\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "            # this will be used in next epoch's gradient update\n",
        "            cluster_assign = result['train']['cluster_assign']\n",
        "\n",
        "            if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
        "                with open(self.result_fname, 'wb') as outfile:\n",
        "                    pickle.dump(results, outfile)\n",
        "                    print(f'result written at {self.result_fname}')\n",
        "                self.save_checkpoint()\n",
        "                print(f'checkpoint written at {self.checkpoint_fname}')\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['train']['loss'] for r in results], label='train')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('loss')\n",
        "        plt.title('Training Loss per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'train_loss.png'))\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot([r['test']['acc'] for r in results], label='train')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('test accuracy')\n",
        "        plt.title('Test Accuracy per Epoch')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(self.config['project_dir'], 'test_acc.png'))\n",
        "\n",
        "\n",
        "    def lr_schedule(self, epoch):\n",
        "        if self.lr is None:\n",
        "            self.lr = self.config['lr']\n",
        "\n",
        "        if epoch % 50 == 0 and epoch != 0 and LR_DECAY:\n",
        "            self.lr = self.lr * 0.1\n",
        "\n",
        "        return self.lr\n",
        "\n",
        "\n",
        "    def print_epoch_stats(self, res):\n",
        "        if res['is_train']:\n",
        "            data_str = 'tr'\n",
        "        else:\n",
        "            data_str = 'tst'\n",
        "\n",
        "        if 'train_time' in res:\n",
        "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
        "        else:\n",
        "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
        "\n",
        "        if 'lr' in res:\n",
        "            lr_str = f\" lr {res['lr']:4f}\"\n",
        "        else:\n",
        "            lr_str = \"\"\n",
        "\n",
        "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} clct{res['cl_ct']}{lr_str} {time_str}\"\n",
        "\n",
        "        print(str0)\n",
        "\n",
        "    def train(self, cluster_assign, lr):\n",
        "        VERBOSE = 0\n",
        "\n",
        "        cfg = self.config\n",
        "        m = cfg['m']\n",
        "        p = cfg['p']\n",
        "        tau = cfg['tau']\n",
        "\n",
        "        # run local update\n",
        "        t0 = time.time()\n",
        "\n",
        "\n",
        "        updated_models = [[None for _ in range(p)] for _ in range(m)]\n",
        "        for m_i in range(m):\n",
        "            if VERBOSE and m_i % 100 == 0: print(f'm {m_i}/{m} processing \\r', end ='')\n",
        "\n",
        "            (X, y) = self.load_data(m_i)\n",
        "\n",
        "            for p_i in cluster_assign[m_i]:\n",
        "\n",
        "                model = copy.deepcopy(self.models[p_i])\n",
        "\n",
        "                # LOCAL UPDATE PER MACHINE tau times\n",
        "                for step_i in range(tau):\n",
        "\n",
        "                    y_logit = model(X)\n",
        "                    loss = self.criterion(y_logit, y)\n",
        "\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.local_param_update(model, lr)\n",
        "\n",
        "                model.zero_grad()\n",
        "\n",
        "                updated_models[m_i][p_i] = model\n",
        "\n",
        "        t02 = time.time()\n",
        "        # print(f'running single ..took {t02-t01:.3f}sec')\n",
        "\n",
        "\n",
        "        t1 = time.time()\n",
        "        if VERBOSE: print(f'local update {t1-t0:.3f}sec')\n",
        "\n",
        "        # apply gradient update\n",
        "        t0 = time.time()\n",
        "\n",
        "        # CLUSTER MACHINES INTO p_i's\n",
        "        local_models = [[] for p_i in range(p)]\n",
        "        for m_i in range(m):\n",
        "            for p_i in cluster_assign[m_i]:\n",
        "                local_models[p_i].append(updated_models[m_i][p_i])\n",
        "\n",
        "        # NEEDS TO BE DECENTRALIZED\n",
        "        for p_i, models in enumerate(local_models):\n",
        "            if len(models) > 0:\n",
        "                self.dec_param_update(models, self.models[p_i])\n",
        "        t1 = time.time()\n",
        "\n",
        "        if VERBOSE: print(f'global update {t1-t0:.3f}sec')\n",
        "\n",
        "    def check_local_model_loss(self, local_models):\n",
        "        # for debugging\n",
        "        m = self.config['m']\n",
        "\n",
        "        losses = []\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i)\n",
        "            y_logit = local_models[m_i](X)\n",
        "            loss = self.criterion(y_logit, y)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        return np.array(losses)\n",
        "\n",
        "\n",
        "    def get_inference_stats(self, train = True):\n",
        "        cfg = self.config\n",
        "        if train:\n",
        "            m = cfg['m']\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            m = cfg['m_test']\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        p = cfg['p']\n",
        "\n",
        "\n",
        "        num_data = 0\n",
        "        losses = {}\n",
        "        corrects = {}\n",
        "        for m_i in range(m):\n",
        "            (X, y) = self.load_data(m_i, train=train) # load batch data rotated\n",
        "\n",
        "            for p_i in range(p):\n",
        "                y_logit = self.models[p_i](X)\n",
        "                loss = self.criterion(y_logit, y) # loss of\n",
        "                n_correct = self.n_correct(y_logit, y)\n",
        "\n",
        "                # if torch.isnan(loss):\n",
        "                #     print(\"nan loss: \", dataset['data_indices'][m_i])\n",
        "\n",
        "                losses[(m_i,p_i)] = loss.item()\n",
        "                corrects[(m_i,p_i)] = n_correct\n",
        "\n",
        "            # print(\"End Size: \", X.shape[0])\n",
        "            num_data += X.shape[0]\n",
        "\n",
        "        # calculate loss and cluster the machines\n",
        "        cluster_assign = [[] for _ in range(m)]\n",
        "        for m_i in range(m):\n",
        "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
        "            min_p_i = np.argmin(machine_losses)\n",
        "            cp_machine_losses = copy.deepcopy(machine_losses)\n",
        "            cp_machine_losses.pop(min_p_i)\n",
        "            sec_min_p_i = np.argmin(cp_machine_losses)\n",
        "            rho = (machine_losses[min_p_i] + 0.2*(abs(machine_losses[min_p_i] - machine_losses[sec_min_p_i])))\n",
        "            cnt = 0\n",
        "            assigned_clusters = set()\n",
        "            while cnt < cfg[\"k'\"]:\n",
        "                for p_i in range(p):\n",
        "                    if machine_losses[p_i] <= rho and p_i not in assigned_clusters:\n",
        "                        cluster_assign[m_i].append(p_i)\n",
        "                        assigned_clusters.add(p_i)\n",
        "                        break\n",
        "                cnt += 1\n",
        "\n",
        "        print(cluster_assign)\n",
        "\n",
        "        # calculate optimal model's loss, acc over all models\n",
        "        min_corrects = []\n",
        "        min_losses = []\n",
        "        for m_i, p_i_list in enumerate(cluster_assign):\n",
        "            for p_i in p_i_list:\n",
        "                min_loss = losses[(m_i, p_i)]\n",
        "                min_losses.append(min_loss)\n",
        "\n",
        "                min_correct = corrects[(m_i, p_i)]\n",
        "                min_corrects.append(min_correct)\n",
        "\n",
        "        # print(\"losses: \", min_losses)\n",
        "        # print(\"corrects: \", min_corrects)\n",
        "        # print(\"num_data: \", num_data)\n",
        "        loss = np.mean(min_losses)\n",
        "        acc = np.sum(min_corrects) / num_data\n",
        "\n",
        "\n",
        "        # check cluster assignment acc\n",
        "        cl_acc = np.mean([set(cluster_assign[m_i]) == set(dataset['cluster_assign'][m_i]) for m_i in range(m)])\n",
        "        cl_ct = [np.sum([p_i in cluster_assign[m_i] for m_i in range(m)]) for p_i in range(p)]\n",
        "\n",
        "        res = {} # results\n",
        "        # res['losses'] = losses\n",
        "        # res['corrects'] = corrects\n",
        "        res['cluster_assign'] = cluster_assign\n",
        "        res['num_data'] = num_data\n",
        "        res['loss'] = loss\n",
        "        res['acc'] = acc\n",
        "        res['cl_acc'] = cl_acc\n",
        "        res['cl_ct'] = cl_ct\n",
        "        res['is_train'] = train\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "        return res\n",
        "\n",
        "    def n_correct(self, y_logit, y):\n",
        "        _, predicted = torch.max(y_logit.data, 1)\n",
        "        correct = (predicted == y).sum().item()\n",
        "\n",
        "        return correct\n",
        "\n",
        "    # TODO Does every Cluster get 4 clients with the same data, but rotated differently?\n",
        "\n",
        "    def load_data(self, m_i, train=True):\n",
        "        # this part is very fast since its just rearranging models\n",
        "        cfg = self.config\n",
        "\n",
        "        if train:\n",
        "            dataset = self.dataset['train']\n",
        "        else:\n",
        "            dataset = self.dataset['test']\n",
        "\n",
        "        indices = dataset['data_indices'][m_i]\n",
        "        p_i = dataset['data_cluster_assign'][m_i]\n",
        "\n",
        "        X_batch = dataset['X'][indices]\n",
        "        y_batch = dataset['y'][indices]\n",
        "\n",
        "        # k : how many times rotate 90 degree\n",
        "        # k =1 : 90 , k=2 180, k=3 270\n",
        "        if cfg['p'] == 4:\n",
        "            k = p_i[0]\n",
        "        elif cfg['p'] == 2:\n",
        "            k = (p_i[0] % 2) * 2\n",
        "        elif cfg['p'] == 1:\n",
        "            k = 0\n",
        "        else:\n",
        "            raise NotImplementedError(\"only p=1,2,4 supported\")\n",
        "\n",
        "        X_batch2 = torch.rot90(X_batch, k=int(k), dims=(1, 2))\n",
        "        X_batch3 = X_batch2.reshape(-1, 28 * 28)\n",
        "\n",
        "        if len(p_i) > 1:\n",
        "            additional_X_batches = []\n",
        "            additional_y_batches = []       \n",
        "\n",
        "            for i in range(1, len(p_i)-1):\n",
        "                additional_rotation = (p_i[0] + i) % 4\n",
        "                X_batch_additional_rot = torch.rot90(X_batch, k=additional_rotation, dims=(1, 2))\n",
        "                X_batch_additional_rot = X_batch_additional_rot.reshape(-1, 28 * 28)\n",
        "                additional_X_batches.append(X_batch_additional_rot)\n",
        "                additional_y_batches.append(y_batch)\n",
        "\n",
        "            X_batch3 = torch.cat([X_batch3] + additional_X_batches, dim=0)\n",
        "            y_batch = torch.cat([y_batch] + additional_y_batches, dim=0)\n",
        "\n",
        "        # print(\"Start Size: \", X_batch3.shape[0])\n",
        "        # print(\"Y_batch size: \", y_batch.shape[0])\n",
        "        return X_batch3, y_batch\n",
        "\n",
        "\n",
        "    def local_param_update(self, model, lr):\n",
        "\n",
        "        # gradient update manually\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data -= lr * param.grad\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace() # we need to check the output of name, check if duplicate exists\n",
        "\n",
        "\n",
        "    def dec_param_update(self, local_models, global_model):\n",
        "\n",
        "        num_clients = len(local_models)\n",
        "\n",
        "        if num_clients == 0:\n",
        "            return\n",
        "\n",
        "        if num_clients == 1:\n",
        "            bc_client = dict(local_models[0].named_parameters())\n",
        "            for name, param in global_model.named_parameters():\n",
        "                param.data = bc_client[name].data.clone()\n",
        "            return\n",
        "\n",
        "        max_e = 100\n",
        "        if num_clients <= max_e:\n",
        "            e = num_clients - 1\n",
        "        else:\n",
        "            e = min(max_e, int(np.log(num_clients) * 10))\n",
        "\n",
        "        if e >= num_clients:\n",
        "            e = num_clients - 1\n",
        "\n",
        "        client_indices = list(range(num_clients))\n",
        "\n",
        "        for m_i, local_model in enumerate(local_models):\n",
        "            selected_clients = random.sample([i for i in client_indices if i != m_i], e)\n",
        "\n",
        "            for m_j in selected_clients:\n",
        "\n",
        "                m_j_params = dict(local_models[m_j].named_parameters())\n",
        "\n",
        "                for name, param in local_model.named_parameters():\n",
        "                    m_i_param = param.data.clone()\n",
        "                    m_j_param = m_j_params[name].data.clone()\n",
        "                    param.data = (m_i_param + m_j_param) / 2\n",
        "\n",
        "        bc_client = random.choice(client_indices)\n",
        "        bc_client_params = dict(local_models[bc_client].named_parameters())\n",
        "        for name, param in global_model.named_parameters():\n",
        "            param.data = bc_client_params[name].data.clone()\n",
        "\n",
        "        # import ipdb; ipdb.set_trace()\n",
        "\n",
        "\n",
        "    def test(self, train=False):\n",
        "        return self.get_inference_stats(train=train)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        models_to_save = [model.state_dict() for model in self.models]\n",
        "        torch.save({'models':models_to_save}, self.checkpoint_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ADsUSUi-tqf"
      },
      "source": [
        "Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T_XDv25r-tqf",
        "outputId": "9c8f4300-c792-4e49-be40-c694fa066e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config: {'m': 60, 'm_test': 8, 'p': 4, 'n': 4000, 'uneven': True, \"k'\": 2, 'h1': 200, 'num_epochs': 300, 'batch_size': 100, 'tau': 10, 'lr': 0.1, 'data_seed': 10, 'train_seed': 10, 'project_dir': 'output'}\n",
            "Using device: cpu\n",
            "m: 60\n",
            "p: 4\n",
            "num_data: 60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "len:  60000\n",
            "m: 8\n",
            "p: 4\n",
            "num_data: 10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "len:  10000\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [1], [2], [1], [1], [2], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [0], [1], [1], [1], [1, 3], [1], [1], [0, 1], [3], [1], [1], [1], [1]]\n",
            "Epoch -1 tr: l 2.298 a 0.110 clct[np.int64(16), np.int64(26), np.int64(2), np.int64(18)] 0.591sec\n",
            "[[0], [0], [3], [3], [1], [1], [1], [1]]\n",
            "Epoch -1 tst: l 2.298 a 0.105 clct[np.int64(2), np.int64(4), np.int64(0), np.int64(2)] 0.109sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [1], [3], [3], [3], [3], [1], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [1], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0, 3], [0], [0], [0], [0]]\n",
            "Epoch 0 tr: l 2.158 a 0.453 clct[np.int64(29), np.int64(4), np.int64(13), np.int64(15)] lr 0.100000 2.316sec(train) 0.577sec(infer)\n",
            "[[0], [0], [3], [1], [2], [1], [0], [0]]\n",
            "Epoch 0 tst: l 2.157 a 0.426 clct[np.int64(4), np.int64(2), np.int64(1), np.int64(1)] 0.100sec\n",
            "result written at output/results.pickle\n",
            "checkpoint written at output/checkpoint.pt\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [2], [3], [3], [3], [3], [2], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 1 tr: l 1.946 a 0.604 clct[np.int64(29), np.int64(1), np.int64(16), np.int64(14)] lr 0.100000 2.210sec(train) 0.573sec(infer)\n",
            "[[0], [0], [3], [2], [2], [2], [0], [0]]\n",
            "Epoch 1 tst: l 1.952 a 0.557 clct[np.int64(4), np.int64(0), np.int64(3), np.int64(1)] 0.106sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [2], [3], [3], [3], [3], [2], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 2 tr: l 1.701 a 0.656 clct[np.int64(29), np.int64(1), np.int64(16), np.int64(14)] lr 0.100000 2.179sec(train) 0.572sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 2 tst: l 1.713 a 0.614 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.101sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 3 tr: l 1.463 a 0.672 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.160sec(train) 0.570sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 3 tst: l 1.483 a 0.625 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.102sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 4 tr: l 1.292 a 0.673 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.194sec(train) 0.587sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 4 tst: l 1.320 a 0.622 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.108sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 5 tr: l 1.187 a 0.673 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.155sec(train) 0.584sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 5 tst: l 1.225 a 0.617 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.098sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 6 tr: l 1.099 a 0.688 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.088sec(train) 0.557sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 6 tst: l 1.127 a 0.645 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.098sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 7 tr: l 1.015 a 0.711 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.139sec(train) 0.575sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 7 tst: l 1.046 a 0.665 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.100sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 8 tr: l 0.956 a 0.722 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.090sec(train) 0.553sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 8 tst: l 0.993 a 0.674 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.097sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 9 tr: l 0.894 a 0.739 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.066sec(train) 0.557sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 9 tst: l 0.934 a 0.690 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.099sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 10 tr: l 0.882 a 0.736 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.102sec(train) 0.560sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 10 tst: l 0.930 a 0.684 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.093sec\n",
            "result written at output/results.pickle\n",
            "checkpoint written at output/checkpoint.pt\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 11 tr: l 0.877 a 0.734 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.128sec(train) 0.548sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 11 tst: l 0.924 a 0.685 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.094sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 12 tr: l 0.816 a 0.753 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.102sec(train) 0.544sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 12 tst: l 0.861 a 0.704 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.100sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 13 tr: l 0.827 a 0.747 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.094sec(train) 0.548sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 13 tst: l 0.879 a 0.695 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.100sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 14 tr: l 0.811 a 0.750 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.078sec(train) 0.595sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 14 tst: l 0.857 a 0.706 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.102sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 15 tr: l 0.811 a 0.750 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.091sec(train) 0.556sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 15 tst: l 0.855 a 0.712 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.099sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 16 tr: l 0.793 a 0.755 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.073sec(train) 0.572sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 16 tst: l 0.837 a 0.717 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.100sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 17 tr: l 0.784 a 0.757 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.061sec(train) 0.551sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 17 tst: l 0.826 a 0.724 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.094sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 18 tr: l 0.700 a 0.782 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.073sec(train) 0.542sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 18 tst: l 0.736 a 0.748 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.098sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 19 tr: l 0.712 a 0.778 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.093sec(train) 0.545sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 19 tst: l 0.736 a 0.758 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.097sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 20 tr: l 0.693 a 0.783 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.054sec(train) 0.552sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 20 tst: l 0.719 a 0.760 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.097sec\n",
            "result written at output/results.pickle\n",
            "checkpoint written at output/checkpoint.pt\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 21 tr: l 0.682 a 0.786 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.018sec(train) 0.560sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 21 tst: l 0.711 a 0.761 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.092sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 22 tr: l 0.677 a 0.787 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.043sec(train) 0.551sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 22 tst: l 0.706 a 0.763 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.098sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 23 tr: l 0.665 a 0.791 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.036sec(train) 0.546sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 23 tst: l 0.688 a 0.771 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.098sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 24 tr: l 0.676 a 0.787 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.052sec(train) 0.542sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 24 tst: l 0.706 a 0.763 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.097sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 25 tr: l 0.624 a 0.802 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.063sec(train) 0.549sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 25 tst: l 0.651 a 0.778 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.096sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 26 tr: l 0.610 a 0.807 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.080sec(train) 0.570sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 26 tst: l 0.632 a 0.786 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.098sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 27 tr: l 0.626 a 0.801 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.085sec(train) 0.551sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 27 tst: l 0.651 a 0.780 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.098sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 28 tr: l 0.616 a 0.803 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.045sec(train) 0.545sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 28 tst: l 0.646 a 0.779 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.094sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 29 tr: l 0.602 a 0.808 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.064sec(train) 0.548sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 29 tst: l 0.632 a 0.783 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.096sec\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 30 tr: l 0.596 a 0.810 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.042sec(train) 0.564sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 30 tst: l 0.618 a 0.793 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.101sec\n",
            "result written at output/results.pickle\n",
            "checkpoint written at output/checkpoint.pt\n",
            "[[0], [0], [0], [0], [0], [3], [0], [0], [0], [0], [0], [0], [0], [0], [0], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [1], [2], [2], [2], [2], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
            "Epoch 31 tr: l 0.603 a 0.807 clct[np.int64(29), np.int64(1), np.int64(14), np.int64(16)] lr 0.100000 2.204sec(train) 0.571sec(infer)\n",
            "[[0], [0], [3], [3], [2], [2], [0], [0]]\n",
            "Epoch 31 tst: l 0.629 a 0.787 clct[np.int64(4), np.int64(0), np.int64(2), np.int64(2)] 0.100sec\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m exp \u001b[38;5;241m=\u001b[39m TrainMNISTCluster(config, device)\n\u001b[1;32m     11\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m duration \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---train cluster Ended in \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;124m hour (\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m sec) \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (duration\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m3600\u001b[39m), duration))\n",
            "Cell \u001b[0;32mIn[49], line 203\u001b[0m, in \u001b[0;36mTrainMNISTCluster.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[1;32m    202\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 203\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_assign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    205\u001b[0m train_time \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m-\u001b[39mt0\n",
            "Cell \u001b[0;32mIn[49], line 315\u001b[0m, in \u001b[0;36mTrainMNISTCluster.train\u001b[0;34m(self, cluster_assign, lr)\u001b[0m\n\u001b[1;32m    312\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(y_logit, y)\n\u001b[1;32m    314\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 315\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_param_update(model, lr)\n\u001b[1;32m    318\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "config = get_config()\n",
        "\n",
        "config['train_seed'] = config['data_seed']\n",
        "\n",
        "print(\"config:\",config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "exp = TrainMNISTCluster(config, device)\n",
        "exp.setup()\n",
        "exp.run()\n",
        "duration = (time.time() - start_time)\n",
        "print(\"---train cluster Ended in %0.2f hour (%.3f sec) \" % (duration/float(3600), duration))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
